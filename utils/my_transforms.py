""" transforms for process data before train/test when using torch"""
import collections
import json
import pickle
import re

import numpy as np
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch

MNIST_LIKE_DATASET = ['MNIST', 'FashionMNIST', 'EMNIST', 'femnist']
CIFAR_LIKE_DATASET = ['CIFAR10', 'CIFAR100', 'SVHN']
# H,W,C -> C,H,W
F_to_tensor = transforms.ToTensor()
# GloVe embeddings for sent 140
Glove6B300dPath = './datasets/processed_data/sent140-mini-embs.json'


def get_transform(dataset):
    """get transform and target transform for dataset"""
    if dataset in MNIST_LIKE_DATASET:
        return mnist_data_transform, int
    elif dataset in CIFAR_LIKE_DATASET:
        return cifar_data_transform, int
    elif dataset == 'synthetic':
        return synthetic_data_transform, int
    elif dataset == 'sent140':
        sent_140 = Sent140_Trans(Glove6B300dPath)
        return sent_140.data_transform, int
    else:
        raise NotImplementedError(f"transform for {dataset} not implemented.")


def mnist_data_transform(raw_x):
    raw_x = np.array(raw_x).reshape((28, 28, 1))
    max_norm = raw_x.max()
    x = F_to_tensor(raw_x).float() / max_norm
    return x


def cifar_data_transform(raw_x):
    raw_x = np.array(raw_x).reshape((32, 32, 3))
    max_norm = raw_x.max()
    x = F_to_tensor(raw_x).float() / max_norm
    return x


def synthetic_data_transform(raw_x):
    return torch.tensor(raw_x)


def sent140_pub_transform(raw_x, seq_len=1):
    # x = np.array(raw_x) * 4320
    # x = np.clip(x, 0, 4319, out=None)
    # return torch.from_numpy(x).int()
    x = torch.tensor(raw_x).float()
    return torch.tile(x, (25, 1))


"""transforms for NLP"""


class Sent140_Trans:

    def __init__(self, glove_6B_300d_path, bag_of_words=False, train_emb_layer=False, max_words=25):
        """ return transforms for sent 140

        Args:
            glove_6B_300d_path: path of `embs.json` generated by glove_6B_300d.
            bag_of_words: bool, if model is using bag_of_words.
            train_emb_layer: bool if rnn models' embedding layers trainable.
                If false, just return the embeddings according to glove_6B_300d.
            max_words: twitter's max words.
        """
        self.word_emb_arr, self.indd, self.vocab = get_word_emb_arr(glove_6B_300d_path)
        self.vocab_size = len(self.vocab)
        self.bag_of_words = bag_of_words
        self.train_emb_layer = train_emb_layer
        self.max_words = max_words

    def data_transform(self, raw_x):
        raw_x = raw_x[4]
        if self.bag_of_words:
            x = bag_of_words(raw_x, self.vocab)
            x = torch.Tensor(x)
        else:
            x = line_to_indices(raw_x, self.indd, self.max_words)
            if self.train_emb_layer:
                x = torch.Tensor(x).int()
            else:
                x = self.word_emb_arr[x]
                x = torch.from_numpy(x).float()
        return x


def get_word_emb_arr(path):
    with open(path, 'r') as inf:
        embs = json.load(inf)
    vocab = embs['vocab']
    word_emb_arr = np.array(embs['emba'])
    indd = {}
    for i in range(len(vocab)):
        indd[vocab[i]] = i
    vocab = {w: i for i, w in enumerate(embs['vocab'])}
    return word_emb_arr, indd, vocab


def bag_of_words(line, vocab):
    '''returns bag of words representation of given phrase using given vocab

    Args:
        line: string representing phrase to be parsed
        vocab: dictionary with words as keys and indices as values

    Return:
        integer list
    '''
    bag = [0] * len(vocab)
    words = split_line(line)
    for w in words:
        if w in vocab:
            bag[vocab[w]] += 1
    return bag


def split_line(line):
    '''split given line/phrase into list of words

    Args:
        line: string representing phrase to be split

    Return:
        list of strings, with each string representing a word
    '''
    return re.findall(r"[\w']+|[.,!?;]", line)


def line_to_indices(line, word2id, max_words=25):
    '''converts given phrase into list of word indices

    if the phrase has more than max_words words, returns a list containing
    indices of the first max_words words
    if the phrase has less than max_words words, repeatedly appends integer
    representing unknown index to returned list until the list's length is
    max_words

    Args:
        line: string representing phrase/sequence of words
        word2id: dictionary with string words as keys and int indices as values
        max_words: maximum number of word indices in returned list

    Return:
        indl: list of word indices, one index for each word in phrase
    '''
    unk_id = len(word2id)
    line_list = split_line(line)
    # split phrase in words
    indl = [word2id[w] if w in word2id else unk_id for w in line_list[:max_words]]
    indl += [unk_id] * (max_words - len(indl))
    return indl
